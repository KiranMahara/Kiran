<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Face Detection and Age Estimation</title>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
<style>
  body {
    font-family: Arial, sans-serif;
    margin: 0;
    padding: 0;
    display: flex;
    justify-content: center;
    align-items: center;
    height: 100vh;
    background-color: #f0f0f0;
  }
  #video {
    max-width: 100%;
    height: auto;
    border: 2px solid #333;
  }
  #overlay {
    position: absolute;
    top: 0;
    left: 0;
    pointer-events: none;
  }
  #info {
    position: absolute;
    bottom: 10px;
    left: 50%;
    transform: translateX(-50%);
    background-color: rgba(0, 0, 0, 0.8);
    color: #fff;
    padding: 10px;
    border-radius: 5px;
    display: none;
  }
</style>
</head>
<body>
<video id="video" autoplay></video>
<canvas id="overlay"></canvas>
<div id="info"></div>
<script>
async function loadModelsAndStart() {
  await Promise.all([
    faceapi.nets.tinyFaceDetector.loadFromUri('models'),
    faceapi.nets.faceLandmark68Net.loadFromUri('models'),
    faceapi.nets.ageGenderNet.loadFromUri('models')
  ]);
  
  startVideo();
}

async function startVideo() {
  const video = document.getElementById('video');
  const overlay = document.getElementById('overlay');
  const info = document.getElementById('info');

  try {
    const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
    video.srcObject = stream;
  } catch (err) {
    console.error('Error accessing the camera:', err);
  }

  video.addEventListener('play', () => {
    const displaySize = { width: video.width, height: video.height };
    faceapi.matchDimensions(overlay, displaySize);

    setInterval(async () => {
      const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
        .withFaceLandmarks()
        .withAgeAndGender();

      const resizedDetections = faceapi.resizeResults(detections, displaySize);
      overlay.getContext('2d').clearRect(0, 0, overlay.width, overlay.height);

      resizedDetections.forEach(detection => {
        const { age, gender, genderProbability, landmarks } = detection;

        new faceapi.draw.DrawTextField(
          [
            `${faceapi.round(age, 0)} years`,
            `${gender} (${faceapi.round(genderProbability)})`
          ],
          detection.detection.box.bottomLeft
        ).draw(overlay);

        faceapi.draw.drawFaceLandmarks(overlay, landmarks);

        overlay.addEventListener('mousemove', (event) => {
          const { x, y } = event;
          if (faceapi.draw.isPointInsideBox({ x, y }, detection.detection.box)) {
            info.style.display = 'block';
            info.style.left = `${x}px`;
            info.style.top = `${y}px`;
            info.innerHTML = `<b>Age:</b> ${faceapi.round(age, 0)} years<br><b>Gender:</b> ${gender} (${faceapi.round(genderProbability)})`;
          } else {
            info.style.display = 'none';
          }
        });
      });
    }, 100);
  });
}

loadModelsAndStart();
</script>
</body>
</html>
